# -*- coding: utf-8 -*-
"""[ZDAIENGpol2] End-to-end ML pipepline with Example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QXE4hDeLMM1WSAr0beoD7A9ICHIEAnCV

https://www.kaggle.com/datasets/ashishjangra27/airbnb-dataset
"""

import pathlib
import os
import zipfile

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

root_dir = pathlib.Path(
    "/content/drive/MyDrive/SDA AI Engineer 2024/ZDAIENGpol2"
)
file_path = root_dir / "Supervised Learning  | sklearn/data/airbnb.csv.zip"
file_path

df_rbb = pd.read_csv(file_path, index_col=0)
df_rbb.head()

df_rbb.columns

df_rbb.groupby('host_id')['name'].nunique().value_counts()

df_host_counts = df_rbb.groupby('host_id')['name'].nunique().rename(
    'host_count'
)

# pd.merge(
#     left=df_rbb,
#     right=df_host_counts,
#     how='left',
#     left_on='host_id',
#     right_index=True
# )

df_rbb['host_count'] = df_host_counts.loc[df_rbb['host_id']].values

cols_to_drop = [
    'id',
    'name',
    # 'rating',
    # 'reviews',
    'host_name',
    'host_id',
    # 'address',
    'features',
    'amenities',
    'safety_rules',
    'hourse_rules',
    'img_links',
    # 'price',
    'country',
    # 'bathrooms',
    # 'beds',
    # 'guests',
    # 'toiles',
    # 'bedrooms',
    # 'studios',
    # 'checkin',
    # 'checkout'
]

df_rbb = df_rbb.drop(columns=cols_to_drop)
df_rbb.head()

df_rbb['address_0'] = df_rbb['address'].str.split(',').str[0]
# df_rbb['address_0']

df_rbb_address_0_counter = df_rbb['address_0'].value_counts().loc[df_rbb['address_0']]

df_rbb['address_0'] = np.where(df_rbb_address_0_counter > 50, df_rbb['address_0'], 'other')

# sns.displot(
#     df_rbb['address'].str.split(',').str[0].value_counts()
# )

# plt.yscale("symlog")

df_rbb['address_0'].value_counts()

# df_rbb['country'].value_counts()

# df_rbb['checkin'].value_counts()
# df_rbb['checkout'].value_counts()

df_rbb = df_rbb.drop(columns=['address', 'checkout', 'checkin'])

df_rbb_rating = np.where(
    df_rbb['rating'] == 'New',
    np.NaN,
    df_rbb['rating']
)

df_rbb_rating_mean = pd.Series(df_rbb_rating.astype(float)).dropna().mean()

df_rbb_rating = np.where(
    pd.Series(df_rbb_rating).isna(),
    df_rbb_rating_mean,
    df_rbb['rating']
).astype(float)

df_rbb['rating'] = df_rbb_rating

df_rbb.hist()

# dla host_count i bedrooms,  beds, bathrooms zmienne odstajace

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# df_rbb = df_rbb.drop(columns=['country'])
# df_rbb.head()

df_rbb['reviews'].nunique()

df_rbb.info()

# df_rbb['reviews'].astype(int)

df_rbb['reviews'] = df_rbb['reviews'].str.replace(',', '').astype(int)

# df_rbb['country'].value_counts()

encoder = OneHotEncoder(sparse_output=False)
df_rbb_address_0_ohe = encoder.fit_transform(df_rbb[['address_0']])
df_rbb_address_0_ohe

df_rbb_address_0_ohe = pd.DataFrame(
    df_rbb_address_0_ohe,
    columns=encoder.get_feature_names_out()
)

df_rbb = pd.concat([df_rbb, df_rbb_address_0_ohe], axis=1)
df_rbb.head()

df_rbb = df_rbb.drop(columns=['address_0'])

df_rbb

X_train, X_test, y_train, y_test = train_test_split(
    df_rbb.drop(columns=['price']),
    df_rbb['price'],
    test_size=0.2,
    random_state=42
)

model_lr = LinearRegression()
model_lr.fit(X_train, y_train)
print(model_lr.score(X_train, y_train))
print(model_lr.score(X_test, y_test))

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model_lr = LinearRegression()
model_lr.fit(X_train_scaled, y_train)
print(model_lr.score(X_train_scaled, y_train))
print(model_lr.score(X_test_scaled, y_test))

df_rbb.iloc[:, :-19]

model_lr = LinearRegression()
model_lr.fit(X_train_scaled[:, :-19], y_train)
print(model_lr.score(X_train_scaled[:, :-19], y_train))
print(model_lr.score(X_test_scaled[:, :-19], y_test))

from sklearn.preprocessing import KBinsDiscretizer

est = KBinsDiscretizer(
    n_bins=7, encode='ordinal', strategy='uniform'
)
X_train.loc[:, 'reviews'] = est.fit_transform(X_train_scaled[:, 1].reshape(-1, 1))
X_test.loc[:, 'reviews'] = est.transform(X_test_scaled[:, 1].reshape(-1, 1))

# pd.DataFrame(output).value_counts()

X_train.iloc[:, :-19].columns

X_test.iloc[:, :-19].columns

model_lr = LinearRegression()
model_lr.fit(X_train.iloc[:, :-19], y_train)
print(model_lr.score(X_train.iloc[:, :-19], y_train))
print(model_lr.score(X_test.iloc[:, :-19], y_test))

X_train.iloc[:, :-20].shape

from sklearn.ensemble import RandomForestRegressor

model_rf = RandomForestRegressor()
model_rf.fit(X_train.iloc[:, :-19], y_train)
print(model_rf.score(X_train.iloc[:, :-19], y_train))
print(model_rf.score(X_test.iloc[:, :-19], y_test))

from sklearn.ensemble import RandomForestRegressor

model_rf = RandomForestRegressor()
model_rf.fit(X_train, y_train)
print(model_rf.score(X_train, y_train))
print(model_rf.score(X_test, y_test))

from sklearn.ensemble import RandomForestRegressor

model_rf = RandomForestRegressor()
model_rf.fit(X_train.iloc[:, :-19], y_train)
print(model_rf.score(X_train.iloc[:, :-19], y_train))
print(model_rf.score(X_test.iloc[:, :-19], y_test))

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score

# Define the parameter grid
param_grid = {
    # 'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the model
rf = RandomForestRegressor()

# Perform grid search with cross-validation
grid_search = GridSearchCV(
    estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='r2'
)
grid_search.fit(X_train.iloc[:, :-19], y_train)

# Print the best parameters and score
print("Best parameters found: ", grid_search.best_params_)
print("Best cross-validation score: ", grid_search.best_score_)

# Evaluate the model with the best parameters on the test set
best_rf = grid_search.best_estimator_
print("Test set score: ", best_rf.score(X_test.iloc[:, :-19], y_test))

best_rf

print(best_rf.score(X_train.iloc[:, :-19], y_train))
print(best_rf.score(X_test.iloc[:, :-19], y_test))

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score

# Define the parameter grid
param_grid = {
    # 'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the model
rf = RandomForestRegressor()

# Perform grid search with cross-validation
grid_search = GridSearchCV(
    estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='r2'
)
grid_search.fit(X_train, y_train)

# Print the best parameters and score
print("Best parameters found: ", grid_search.best_params_)
print("Best cross-validation score: ", grid_search.best_score_)

# Evaluate the model with the best parameters on the test set
best_rf = grid_search.best_estimator_
print("Test set score: ", best_rf.score(X_test, y_test))

print(best_rf.score(X_train, y_train))
print(best_rf.score(X_test, y_test))

from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

model_lr = LinearRegression()
model_lr.fit(X_train_poly, y_train)
print(model_lr.score(X_train_poly, y_train))
print(model_lr.score(X_test_poly, y_test))

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_train_poly_pca_2 = pca.fit_transform(X_train_poly)
X_test_poly_pca_2 = pca.transform(X_test_poly)

model_lr = LinearRegression()
model_lr.fit(X_train_poly_pca_2, y_train)

print(model_lr.score(X_train_poly_pca_2, y_train))
print(model_lr.score(X_test_poly_pca_2, y_test))

from sklearn.decomposition import PCA

pca = PCA(n_components=10)
X_train_poly_pca_2 = pca.fit_transform(X_train)
X_test_poly_pca_2 = pca.transform(X_test)

model_lr = LinearRegression()
model_lr.fit(X_train_poly_pca_2, y_train)

print(model_lr.score(X_train_poly_pca_2, y_train))
print(model_lr.score(X_test_poly_pca_2, y_test))

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score

# Define the parameter grid
param_grid = {
    # 'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the model
rf = RandomForestRegressor()

# Perform grid search with cross-validation
grid_search = GridSearchCV(
    estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='r2'
)
grid_search.fit(X_train_poly_pca_2, y_train)

# Print the best parameters and score
print("Best parameters found: ", grid_search.best_params_)
print("Best cross-validation score: ", grid_search.best_score_)

# Evaluate the model with the best parameters on the test set
best_rf = grid_search.best_estimator_
print("Test set score: ", best_rf.score(X_test_poly_pca_2, y_test))

sns.boxenplot(y_train)

plt.yscale('log')

from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier

pca = PCA(n_components=10)
X_train_poly_pca_2 = pca.fit_transform(X_train_poly)
X_test_poly_pca_2 = pca.transform(X_test_poly)

model_lr = KNeighborsClassifier(n_neighbors=2)
model_lr.fit(X_train_poly_pca_2, y_train)

print(model_lr.score(X_train_poly_pca_2, y_train))
print(model_lr.score(X_test_poly_pca_2, y_test))

